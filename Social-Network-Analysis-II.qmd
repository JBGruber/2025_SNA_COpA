---
title: "Social Network Analysis 2"
subtitle: "COST Action Training School in Computational Opinion Analysis -- COpA"
footer: "COpA -- 28th May 2025, Elbasan"
author: "Johannes B. Gruber | GESIS"
format:
  html: default
execute: 
  eval: true
  freeze: true
---

# Example: Divided They Blog

## Packages and setup

```{r setup}
#| include: false
library(tidyverse); theme_set(theme_minimal())
library(igraph)
library(tidygraph)
library(ggraph)
library(atrrr)
```

```{r}
if (!reticulate::condaenv_exists("r-copa")) {
  packages <- c(
    "python=3.11",
    "networkx",
    "igraph",
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    # "community",
    "cdlib",
    "imageio",
    "leidenalg",
    "graph-tool",
    "tqdm"
  )
  try(reticulate::install_miniconda(update = TRUE), silent = TRUE)
  reticulate::conda_install("r-copa", packages = packages)
}
reticulate::use_condaenv("r-copa")
```

```{python}
import networkx as nx
import igraph as ig
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.patches as mpatches
from matplotlib.lines import Line2D
import seaborn as sns
import requests
import zipfile
import io
import os
import imageio.v2 as imageio
from tqdm import tqdm
import leidenalg
```

## The Blogosphere data

```{r}
graph_file <- "data/polblogs.zip"
dir.create(dirname(graph_file), showWarnings = FALSE)
if (!file.exists(graph_file)) {
  curl::curl_download(
    "https://public.websites.umich.edu/~mejn/netdata/polblogs.zip",
    graph_file
  )
}

# have a quick look at the data description
unz(graph_file, "polblogs.txt") |> 
  readLines() |> 
  cat(sep = "\n")
```

```{python}
graph_file = "data/polblogs.zip"
os.makedirs(os.path.dirname(graph_file), exist_ok=True)

if not os.path.exists(graph_file):
    print("Downloading polblogs.zip file...")
    url = "https://public.websites.umich.edu/~mejn/netdata/polblogs.zip"
    response = requests.get(url)
    with open(graph_file, 'wb') as f:
        f.write(response.content)
    print("Download complete.")

# have a quick look at the data description
with zipfile.ZipFile(graph_file, 'r') as zip_ref:
    with zip_ref.open('polblogs.txt') as f:
        print(f.read().decode('utf-8'))
```


## graph data structure in `R`

Let's first look at the `igraph` graph class:

```{r}
polblogs_igraph <- igraph::read_graph(unz(graph_file, "polblogs.gml"), format = "gml") 
class(polblogs_igraph)
polblogs_igraph
```

```{python}
with zipfile.ZipFile(graph_file, 'r') as zip_ref:
    with zip_ref.open('polblogs.gml') as f:
        gml_content = f.read().decode('utf-8')
        with open("data/polblogs.gml", 'w') as f:
            f.write(gml_content)

polblogs_ig = ig.Graph.Read_GML("data/polblogs.gml")

print(type(polblogs_ig))
print(polblogs_ig.summary())
```

## graph data structure in `R`

We can convert this to a `tidygraph` object:

```{r}
polblogs_tbl <- as_tbl_graph(polblogs_igraph)
class(polblogs_tbl)
polblogs_tbl
```

```{python}
# Python equivalent using NetworkX for tidygraph-like functionality
polblogs_nx = ig.Graph.to_networkx(polblogs_ig)
print(type(polblogs_nx))
print(f"NetworkX Graph with {polblogs_nx.number_of_nodes()} nodes and {polblogs_nx.number_of_edges()} edges")
print(f"Directed: {polblogs_nx.is_directed()}")
```

## graph data structure in `R`/`Python`

Both `igraph` and `tbl_graph` objects essentially consist of two linked tables containing [nodes]{.kn-pink} (aka vertices) and [edges]{.kn-pink}.

```{r}
polblogs_tbl |> 
  activate("nodes") |> 
  as_tibble()
```
```{python}
nodes_df = pd.DataFrame({
    'id': range(polblogs_ig.vcount()),
    'value': polblogs_ig.vs['value'],
    'name': polblogs_ig.vs['label']
})
print("\nNodes:")
print(nodes_df.head())
```


```{r}
polblogs_tbl |> 
  activate("edges") |> 
  as_tibble()
```

```{python}
edges_df = pd.DataFrame({
    'from': [e.source for e in polblogs_ig.es],
    'to': [e.target for e in polblogs_ig.es]
})
print("\nEdges:")
print(edges_df.head())
```


## working with graph data structures in `R`

We can look up the variables (like political class) for any given node by filtering.
For example, let's see the node with ID 30:

```{r}
polblogs_tbl |> 
  filter(id == 30)
```

```{python}
for vertex in polblogs_ig.vs:
    if vertex['id'] == 30:
        print(vertex)
```


## working with graph data structures in `R`

If we want to see how many left and right blogs are in the data, we can use `count`.
But only after converting the nodes to a `data.frame`!

```{r}
polblogs_tbl |>
  activate("nodes") |>
  as_tibble() |> 
  count(ideology = value)
```


```{python}
left_blogs = []
right_blogs = []
for vertex in polblogs_ig.vs:
    if vertex['value'] == 0:
        left_blogs.append(vertex)
    else:
        right_blogs.append(vertex)

print(f"{len(left_blogs)} left blogs")
print(f"{len(right_blogs)} right blogs")
```

## working with graph data structures in `R`

The `value` variable is named terribly and stored in a strange format.
Let's change that using some more tidyverse functions that work out-of-the-box with `tidygraph` graphs:

```{r}
polblogs_tbl_new <- polblogs_tbl |> 
  activate("nodes") |> 
  mutate(ideology = recode_factor(value, 
                                  "0" = "left",
                                  "1" = "right"))
polblogs_tbl_new
```

```{python}
ideology = []
for vertex in polblogs_ig.vs:
    if vertex['value'] == 0:
        ideology.append("left")
    else:
        ideology.append("right")

polblogs_ig.vs['ideology'] = ideology

print(polblogs_ig.vs[1])
```

:::{.notes}
It's not necessary to store ideology as a factor, but generally good practice in `R`.
Whenever you have a variable with just a few repeating character values, a factor is more efficient.
:::

## first insights

We can answer some initial questions about the data:

- how many left and right blogs are there? 

```{r}
polblogs_tbl_new |> 
  as_tibble() |> 
  count(ideology)
```

```{python}
polblogs_ig.vs['ideology'].count('left')
polblogs_ig.vs['ideology'].count('right')
```

## first insights

We can answer some initial questions about the data:

- how many connections (edges) do nodes have to other nodes?

```{r}
polblogs_tbl_new |> 
  activate(edges) |> 
  as_tibble() |> 
  count(from, sort = TRUE)
```
```{python}
from_counts = edges_df['from'].value_counts().head(10)
print("Top nodes by outgoing connections:")
print(from_counts)
```



Let's have a closer look at the top node:

```{r}
polblogs_tbl_new |> 
  filter(id == 855) |> 
  as_tibble()
```

```{python}
print(nodes_df[nodes_df['id'] == 854])
```


```{r}
polblogs_tbl_new |> 
  activate(edges) |> 
  as_tibble() |> 
  count(to, sort = TRUE)
```

```{python}
to_counts = edges_df['to'].value_counts().head(10)
print("\nTop nodes by incoming connections:")
print(to_counts)
```


Let's have a closer look at the top node:

```{r}
polblogs_tbl_new |> 
  filter(id == 155) |> 
  as_tibble()
```


```{python}
print(nodes_df[nodes_df['id'] == 154])
```

## first insights

We can answer some initial questions about the data:

- do left and right blogs reference each other?

```{r}
polblogs_tbl_new |> 
  activate(edges) |> 
  mutate(from_ideo = .N()$ideology[from],
         to_ideo = .N()$ideology[to]) |> 
  as_tibble() |> 
  count(from_ideo, to_ideo)
```

```{python}
nodes_df = pd.DataFrame({
    'id': range(polblogs_ig.vcount()),
    'value': polblogs_ig.vs['value'],
    'name': polblogs_ig.vs['label'],
    'ideology': polblogs_ig.vs['ideology']
}) 

# Map ideology to from and to nodes
edges_df['from_ideo'] = edges_df['from'].map(nodes_df['ideology'])
edges_df['to_ideo'] = edges_df['to'].map(nodes_df['ideology'])

# Count combinations of from_ideo and to_ideo
print(edges_df.groupby(['from_ideo', 'to_ideo']).size().reset_index(name='n'))
```

## visualising graphs in `R`

- `ggraph` inherits the idea of a grammar of graphics from `ggplot`
- hence, we build up plots in layers with visual aesthetics mapped to data
- the difference is, we address map data from the nodes and edges table

![ggraph](https://ggraph.data-imaginist.com/reference/figures/logo.png)


```{r}
ggraph(graph = polblogs_tbl_new, layout = "kk") + 
  geom_edge_link() + 
  geom_node_point(aes(colour = ideology))
```

## visualising graphs in `Python`

- Basic network visualization using NetworkX and matplotlib

```{python}
plt.figure(figsize=(12, 8))

# Get node colors based on ideology
node_colors = ['#2F357E' if polblogs_ig.vs[i]['ideology'] == 'left' else '#D72F32'
               for i in range(len(polblogs_ig.vs))]

nx.draw_kamada_kawai(polblogs_nx, with_labels=False, node_color=node_colors)
plt.show()
```

## Recreate the plot from Adamic and Glance (2005)

```{r}
#| output-location: column
polblogs_tbl_new |> 
  # you can sample the graph to make plotting quicker (but incomplete)
  # sample_frac(0.1) |> 
  # the size of the bubbles is influenced by the number of blogs that link to it
  mutate(referenced = centrality_degree(mode = "in", loops = FALSE)) |> 
  # remove isolated nodes
  activate(nodes) |>
  filter(!node_is_isolated()) |>
  # the colour of edges is influenced by whether the connection is left, right 
  # or bipartisan
  activate(edges) |> 
  mutate(col = case_when(
    .N()$ideology[from] == "left" & .N()$ideology[to] == "left" ~ "#2F357E",
    .N()$ideology[from] == "right" & .N()$ideology[to] == "right" ~ "#D72F32",
    .N()$ideology[from] == "left" & .N()$ideology[to] == "right" ~ "#f4c23b",
    .N()$ideology[from] == "right" & .N()$ideology[to] == "left" ~ "#f4c23b"
  )) |> 
  # the stress majorization algorithm in ggraph is the closed to the original
  # force directed layout from
  ggraph(layout = "stress") +
  geom_edge_link(aes(colour = col),
                 arrow = arrow(length = unit(2, "mm"), type = "closed")) +
  # we map the number of references to the size
  geom_node_point(aes(fill = ideology, size = referenced),
                  # black border and a different shape creates bubbles
                  colour = "black", pch = 21) +
  scale_fill_manual(values = c(left = "#2F357E", right = "#D72F32")) +
  scale_edge_colour_identity() + 
  theme_graph()
```

```{python}
plt.figure(figsize=(15, 12))

# Calculate in-degree centrality (referenced)
in_degrees = polblogs_nx.in_degree()
referenced = {node: degree for node, degree in in_degrees}

# Remove isolated nodes
non_isolated_nodes = [node for node in polblogs_nx.nodes() if polblogs_nx.degree(node) > 0]
subgraph = polblogs_nx.subgraph(non_isolated_nodes)

# Prepare edge colors based on ideology connections
edge_colors = []
for edge in subgraph.edges():
    from_node, to_node = edge
    from_ideo = polblogs_ig.vs[from_node]['ideology']
    to_ideo = polblogs_ig.vs[to_node]['ideology']
    
    if from_ideo == "left" and to_ideo == "left":
        edge_colors.append("#2F357E")
    elif from_ideo == "right" and to_ideo == "right":
        edge_colors.append("#D72F32")
    else:  # bipartisan connections
        edge_colors.append("#f4c23b")

# Prepare node properties
node_colors = ['#2F357E' if polblogs_ig.vs[node]['ideology'] == 'left' else '#D72F32' 
               for node in subgraph.nodes()]
node_sizes = [referenced.get(node, 1) * 10 + 20 for node in subgraph.nodes()]

# Draw the network
nx.draw_kamada_kawai(subgraph, with_labels=False, node_color=node_colors,
                     edge_color=edge_colors)

plt.show()
```

## volatile layouts

One thing that always makes me cautious about interpreting network plots is how volatile the placement of nodes in the plot is and how much it can trick you into finding a pattern where none exists.
So let's look at an experiment:

Prepare data for plotting:

```{r}
set.seed(1)
plot_data <- polblogs_tbl_new |> 
  # you can sample the graph to make plotting quicker (but incomplete)
  sample_frac(0.25) |> 
  mutate(referenced = centrality_degree(mode = "in", loops = FALSE)) |> 
  activate(nodes) |>
  filter(!node_is_isolated()) |>
  activate(edges) |> 
  mutate(col = case_when(
    .N()$ideology[from] == "left" & .N()$ideology[to] == "left" ~ "#2F357E",
    .N()$ideology[from] == "right" & .N()$ideology[to] == "right" ~ "#D72F32",
    .N()$ideology[from] == "left" & .N()$ideology[to] == "right" ~ "#f4c23b",
    .N()$ideology[from] == "right" & .N()$ideology[to] == "left" ~ "#f4c23b"
  ))
```

```{python}
np.random.seed(1)

# Sample 25% of nodes randomly for faster plotting
sample_size = int(0.25 * polblogs_nx.number_of_nodes())
sampled_nodes = np.random.choice(list(polblogs_nx.nodes()), size=sample_size, replace=False)
plot_subgraph = polblogs_nx.subgraph(sampled_nodes)

# Remove isolated nodes
non_isolated = [node for node in plot_subgraph.nodes() if plot_subgraph.degree(node) > 0]
plot_subgraph = plot_subgraph.subgraph(non_isolated)

# Calculate referenced (in-degree)
in_degrees_sample = plot_subgraph.in_degree()
```

Get all available layouts:

```{r}
layouts <- c(
  "auto",       # Automatic layout
  "circle",     # Circular layout
  "dh",         # Davidson-Harel layout
  "drl",        # Distributed Recursive Layout
  "fr",         # Fruchterman-Reingold layout
  "gem",        # GEM layout
  "graphopt",   # Graphopt layout
  "grid",       # Grid layout
  "kk",         # Kamada-Kawai layout
  "lgl",        # Large Graph Layout
  "linear",     # Linear layout
  "mds",        # Multidimensional Scaling layout
  "randomly",   # Random layout
  "sphere",     # Spherical layout
  "star",       # Star layout
  "stress",     # Stress majorization layout
  "sugiyama",   # Sugiyama layout (for layered graphs)
  "tree"        # Tree layout
)
```


```{python}
nx_layouts = {
    'circular': nx.circular_layout,
    'random': nx.random_layout,
    'shell': nx.shell_layout,
    'spring': nx.spring_layout,
    'kamada_kawai': nx.kamada_kawai_layout,
    'planar': nx.planar_layout,
    'spectral': nx.spectral_layout
}
```

## volatile layouts

```{r layouts}
#| output-location: column
dir.create("media/layouts/")
for (layout in layouts) {
  
  # plot status message in interactive sessions
  if (interactive()) {
    message("plotting using layout ", layout)
  }
  
  plot_f <- paste0("media/layouts/network_", layout, ".png")
  
  if (!file.exists(plot_f)) {
    p <- plot_data |> 
      ggraph(layout = layout) +
      geom_edge_link(aes(colour = col),
                     arrow = arrow(length = unit(2, "mm"), type = "closed")) +
      # we map the number of references to the size
      geom_node_point(aes(fill = ideology, size = referenced),
                      # black border and a different shape creates bubbles
                      colour = "black", pch = 21) +
      scale_fill_manual(values = c(left = "#2F357E", right = "#D72F32")) +
      scale_edge_colour_identity() + 
      theme_graph() +
      labs(caption = paste0("Layout: ", layout))
    ggsave(filename = plot_f, plot = p, width = 7, height = 7)
  }
  
}
gif_file <- list.files("media/layouts/", full.names = TRUE) |> 
  gifski::gifski(gif_file = "media/layouts.gif")
knitr::include_graphics(gif_file)
```

```{python}
#| eval: false
# Create plots with different layouts to show volatility
os.makedirs("media/layouts_py/", exist_ok=True)

def create_network_plot(subgraph, layout_func, layout_name, save_path):
    plt.figure(figsize=(10, 8))
    
    try:
        if layout_name == 'planar':
            # Check if graph is planar
            if nx.is_planar(subgraph):
                pos = layout_func(subgraph)
            else:
                pos = nx.spring_layout(subgraph)  # fallback
        else:
            pos = layout_func(subgraph)
    except:
        pos = nx.spring_layout(subgraph)  # fallback for any errors
    
    # Prepare edge colors
    edge_colors = []
    for edge in subgraph.edges():
        from_node, to_node = edge
        if from_node < len(polblogs_ig.vs) and to_node < len(polblogs_ig.vs):
            from_ideo = polblogs_ig.vs[from_node]['ideology']
            to_ideo = polblogs_ig.vs[to_node]['ideology']
            
            if from_ideo == "left" and to_ideo == "left":
                edge_colors.append("#2F357E")
            elif from_ideo == "right" and to_ideo == "right":
                edge_colors.append("#D72F32")
            else:
                edge_colors.append("#f4c23b")
        else:
            edge_colors.append("#gray")
    
    # Prepare node properties
    node_colors = []
    node_sizes = []
    for node in subgraph.nodes():
        if node < len(polblogs_ig.vs):
            ideology = polblogs_ig.vs[node]['ideology']
            node_colors.append('#2F357E' if ideology == 'left' else '#D72F32')
            node_sizes.append(in_degrees_sample.get(node, 1) * 20 + 30)
        else:
            node_colors.append('gray')
            node_sizes.append(30)
    
    # Draw network
    nx.draw_kamada_kawai(subgraph, with_labels=False, node_color=node_colors,
                         edge_color=edge_colors)
    # nx.draw_networkx_edges(subgraph, pos, edge_color=edge_colors, alpha=0.6, 
    #                       width=0.5, arrows=True, arrowsize=8)
    # nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, 
    #                       node_size=node_sizes, alpha=0.8, edgecolors='black')
    
    plt.title(f"Layout: {layout_name}", fontsize=14)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

# Generate plots for different layouts
layout_files = []
for layout_name, layout_func in nx_layouts.items():
    plot_path = f"media/layouts_py/network_{layout_name}.png"
    print(plot_path)
    create_network_plot(plot_subgraph, layout_func, layout_name, plot_path)
    layout_files.append(plot_path)
    print(f"Created plot with {layout_name} layout")

# Create GIF from the images
if layout_files:
    images = [imageio.imread(f) for f in layout_files if os.path.exists(f)]
    if images:
        imageio.mimsave("media/layouts_python.gif", images, duration=1.5)
        print("Created animated GIF showing layout volatility")
```

## community detection

In the previous figure we used the political orientation of blogs manually assigned by Adamic and Glance (2005).
Usually, we want to detect communities from the network structure itself.
We learned about the [Louvain]{.kn-pink} and [Leiden]{.kn-pink} algorithms (and about their downsides).
So let's start with these.

```{r}
#| output-location: column
polblogs_tbl_new_grouped <- polblogs_tbl_new |> 
  activate(nodes) |>
  to_undirected() |> 
  mutate(group_louvain = group_louvain(),
         group_leiden = group_leiden())

polblogs_tbl_new_grouped |> 
  as_tibble()
```

```{python}
# Community detection using Louvain and Leiden algorithms
import leidenalg
import networkx.algorithms.community as nx_comm

# Convert to undirected graph for community detection
polblogs_undirected = polblogs_nx.to_undirected()

# Louvain community detection using NetworkX
louvain_communities = nx_comm.louvain_communities(polblogs_undirected, seed=42)

# Create group assignments for Louvain
group_louvain = {}
for i, community in enumerate(louvain_communities):
    for node in community:
        group_louvain[node] = i

# Leiden community detection using igraph and leidenalg
leiden_partition = leidenalg.find_partition(polblogs_ig, leidenalg.ModularityVertexPartition)

# Create group assignments for Leiden
group_leiden = {}
for i, community in enumerate(leiden_partition):
    for node in community:
        group_leiden[node] = i

# Add community assignments to nodes dataframe
nodes_df['group_louvain'] = nodes_df['id'].map(group_louvain)
nodes_df['group_leiden'] = nodes_df['id'].map(group_leiden)

print("Nodes with community assignments:")
print(nodes_df.head(10))

print(f"\nNumber of Louvain communities: {len(louvain_communities)}")
print(f"Number of Leiden communities: {len(set(leiden_partition.membership))}")
```

## community detection

```{r}
#| output-location: column
polblogs_tbl_new_grouped |> 
  activate(nodes) |>
  mutate(group_louvain = factor(group_louvain()),
         group_leiden = factor(group_leiden())) |> 
  # the size of the bubbles is influenced by the number of blogs that link to it
  mutate(referenced = centrality_degree(mode = "in", loops = FALSE)) |> 
  # remove isolated nodes
  filter(!node_is_isolated()) |>
  ggraph(layout = "stress") +
  geom_edge_link(colour = "gray",
                 arrow = arrow(length = unit(2, "mm"), type = "closed")) +
  # we map the number of references to the size
  geom_node_point(aes(fill = group_louvain, size = referenced),
                  # black border and a different shape creates bubbles
                  colour = "black", pch = 21) +
  theme_graph() +
  labs(title = "Blogosphere grouped by Louvain")
```

```{python}
# Plot network with Louvain communities
plt.figure(figsize=(15, 12))

# Create layout
non_isolated_nodes = [node for node in polblogs_undirected.nodes() if polblogs_undirected.degree(node) > 0]
subgraph_undirected = polblogs_undirected.subgraph(non_isolated_nodes[1:100])
# pos = nx.draw_kamada_kawai(subgraph_undirected)

# Prepare node colors based on Louvain communities
louvain_colors = plt.cm.Set3(np.linspace(0, 1, len(louvain_communities)))
node_colors_louvain = []
for node in subgraph_undirected.nodes():
    community_id = group_louvain.get(node, 0)
    node_colors_louvain.append(louvain_colors[community_id % len(louvain_colors)])

# Prepare node sizes based on in-degree
in_degrees_full = polblogs_nx.in_degree()

# Draw the network
nx.draw_kamada_kawai(subgraph_undirected, edge_color='black',
                     width=0.2, arrows=True, arrowsize=8, arrowstyle='->',
                     node_color=node_colors_louvain)

plt.title("Blogosphere grouped by Louvain", fontsize=16)
plt.axis('off')
plt.tight_layout()
plt.show()
```

## community detection

```{r}
#| output-location: column
polblogs_tbl_new_grouped |> 
  activate(nodes) |>
  mutate(group_louvain = factor(group_louvain()),
         group_leiden = factor(group_leiden())) |> 
  # the size of the bubbles is influenced by the number of blogs that link to it
  mutate(referenced = centrality_degree(mode = "in", loops = FALSE)) |> 
  # remove isolated nodes
  filter(!node_is_isolated()) |>
  ggraph(layout = "stress") +
  geom_edge_link(colour = "gray",
                 arrow = arrow(length = unit(2, "mm"), type = "closed")) +
  # we map the number of references to the size
  geom_node_point(aes(fill = group_leiden, size = referenced),
                  # black border and a different shape creates bubbles
                  colour = "black", pch = 21, show.legend = FALSE) +
  theme_graph() +
  labs(title = "Blogosphere grouped by Leiden")
```

```{python}
#| eval: false
# Plot network with Leiden communities
plt.figure(figsize=(15, 12))

# Use the same layout as before for comparison
pos = nx.draw_kamada_kawai(subgraph_undirected)

# Prepare node colors based on Leiden communities
n_leiden_communities = len(set(leiden_partition.membership))
leiden_colors = plt.cm.Set1(np.linspace(0, 1, n_leiden_communities))
node_colors_leiden = []
for node in subgraph_undirected.nodes():
    community_id = group_leiden.get(node, 0)
    node_colors_leiden.append(leiden_colors[community_id % len(leiden_colors)])

# Use same node sizes as before
# node_sizes_leiden = [in_degrees_full.get(node, 1) * 15 + 30 for node in subgraph_undirected.nodes()]

# Draw the network
nx.draw_networkx_edges(subgraph_undirected, pos, edge_color='gray', alpha=0.3, width=0.2,
                      arrows=True, arrowsize=8, arrowstyle='->')
nx.draw_networkx_nodes(subgraph_undirected, pos, node_color=node_colors_leiden, 
                       alpha=0.8, edgecolors='black', linewidths=0.5)

plt.title("Blogosphere grouped by Leiden", fontsize=16)
plt.axis('off')
plt.tight_layout()
plt.show()
```


# Bluesky: What is my Bluesky network?

Before we start working with Bluesky, you should authenticate your session:

```{r}
#| eval: false
auth("jbgruber.bsky.social")
```

When you are new to Bluesky you should start by searching a few names that you know.
There are also starter packs like <https://bsky.app/starter-pack/sof14g1l.bsky.social/3lbc4bqetfp22> which you can follow.
But what then?
Given the idea of homophily, you might want to check who the people you follow, follow themselves.
So let's get started with that (replace my handle with yours below if you like):

```{r}
my_follows <- get_follows(actor = "jbgruber.bsky.social", limit = Inf)
my_follows |>
  glimpse()
```

Next, we want to see who they follow:

```{r}
#| eval: !expr '!file.exists("data/their_follows.rds")'
their_follows <- my_follows$actor_handle |> 
  # the 50 accounts at the bottom of the list are the ones I followed first
  tail(50) |> 
  map(function(handle) {
  # for demo purposes I add a limit since some follow 1000s
  tibble(
    from = handle,
    to =  get_follows(handle, limit = 1000L, verbose = FALSE)$actor_handle
  )
}, .progress = TRUE) |> 
  bind_rows()
saveRDS(their_follows, "data/their_follows.rds")
```

```{r}
their_follows <- readRDS("data/their_follows.rds")
```


As a first step, we can just check the raw number of who shows up most often here:

```{r}
their_follows |> 
  count(to) |> 
  filter(to != "handle.invalid") |> 
  slice_max(order_by = n, n = 15) |> 
  mutate(to = fct_reorder(to, n)) |> 
  ggplot(aes(x = n, y = to)) +
  geom_col()
```

But we can also use network analysis to find influential accounts:


```{r}
follows_graph <- as_tbl_graph(their_follows, directed = TRUE) |> 
  activate(nodes) %>%
  mutate(
    degree = centrality_degree(),
    closeness = centrality_closeness(),
    betweeness = centrality_betweenness(),
    eigenvector = centrality_eigen()
  )
```

Let's look at **degree** centrality (simply counts the number of neighbors a node has):

```{r}
follows_graph |> 
  as_tibble() |> 
  arrange(degree) |> 
  slice_max(degree, n = 10)
```

 **closeness** (computes the shortest path distances among nodes. The most central node has the minimum distance to all other nodes)
 
```{r}
follows_graph |> 
  as_tibble() |> 
  arrange(closeness) |> 
  slice_max(closeness, n = 10)
```

**betweeness** (number of shortest paths that pass through a node, divided by the total number of shortest paths) 

```{r}
central_accounts <- follows_graph |> 
  as_tibble() |> 
  arrange(betweeness) |> 
  slice_max(betweeness, n = 10)
central_accounts
```

and **eigenvector centrality** (extends the idea of degree by assuming that a node is central if it is connected to other central nodes)

```{r}
follows_graph |> 
  as_tibble() |> 
  arrange(eigenvector) |> 
  slice_max(eigenvector, n = 10)
```

We can also visialise this network and highlight some of the accounts in it:

```{r}
follows_graph |> 
  mutate(central_account = name %in% central_accounts$name) |> 
  slice_max(eigenvector, n = 500) |> 
  ggraph(layout = "mds") +
  geom_edge_link(alpha = 0.7) +
  geom_node_point(aes(size = betweeness, color = central_account)) +
  scale_color_manual(values = c("TRUE" = "firebrick", "FALSE" = "lightblue")) +
  geom_node_label(aes(label = ifelse(central_account, name, NA)), vjust = 1, hjust = 1) +
  theme_graph() +
  coord_equal(clip = "off")
```

For a different approach to find people to follow, you can check out <https://www.johannesbgruber.eu/post/2024-11-24-bluesky-rising/>.


# Bluesky: Checking out the #rstats network


First let's get some data.
The code below searches for posts that mention the hashtag #rstats, which is widely use for all things R:

```{r}
#| eval: !expr '!file.exists("data/rstats_content.rds")'
rstats_content <- search_skeet("#rstats", since = "2025-04-01", limit = Inf)
saveRDS(rstats_content, "data/rstats_content.rds")
```

```{r}
rstats_content <- readRDS("data/rstats_content.rds")
```


## Semantic Network/Co-hashtag Network

The first network we can build from this data is a co-occurence network.
We check, which hashtags are used together in these posts.
In other words: the hashtags are nodes, and the edges are whether the hashtags were used together.
As a first step, let's get some info on the most popular hashtags:

```{r}
rstats_tags <- rstats_content |> 
  unnest_longer(tags) |> 
  mutate(hashtag = tolower(tags))

rstats_tags_count <- rstats_tags |> 
  count(hashtag)

rstats_tags_count |> 
  slice_max(order_by = n, n = 10) |> 
  mutate(hashtag = fct_reorder(hashtag, n)) |> 
  ggplot(aes(x = n, y = hashtag)) +
  geom_col()
```

Now, to find co-occurrence of hashtags.
You could also do this with words, but it often comes out less meaningful.

```{r}
rstats_tags_cooc <- rstats_tags |> 
  # for each post (or rather for each unique post ID), we get all possible 
  # combinations of hashtags
  group_by(cid) |> 
  group_map(function(df, ...) expand_grid(from = df$hashtag, to = df$hashtag)) |> 
  bind_rows() |> 
  # we filter out cases where the hashtag 'coocurrs' with itself
  filter(from != to)
rstats_tags_cooc
```

Now you should notice this looks like a network already.
Let's make it one:

```{r}
rstats_tags_network <- tbl_graph(edges = rstats_tags_cooc, directed = FALSE) |> 
  activate(nodes) |> 
  left_join(rstats_tags_count, by = c("name" = "hashtag"))
rstats_tags_network
```

We can try a few things to turn this into a nice plot.
Let's get a baseline first:

```{r}
rstats_tags_network |> 
  activate(nodes) |> 
  # to make it easier to look at, let's limit ourselves to top hashtags
  slice_max(order_by = n, n = 50) |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  # instead of nodes, I use labels directly here
  geom_node_label(aes(label = name)) +
  theme_graph()
```

We can use color to show the centrality of nodes.
You can play around with which measure produces the most interesting visual.

```{r}
rstats_tags_network |> 
  activate(nodes) |> 
  mutate(
    degree = centrality_degree(),
    closeness = centrality_closeness(),
    betweeness = centrality_betweenness(),
    eigenvector = centrality_eigen()
  ) |> 
  slice_max(order_by = n, n = 50) |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  geom_node_label(aes(label = name, fill = eigenvector)) +
  scale_fill_continuous(low = "lightblue", high = "firebrick") +
  theme_graph()
```

We can also see if we can find some communities in this data.
As above, we are using the Louvain and Leiden algorithms:

```{r}
rstats_tags_network_communities <- rstats_tags_network |> 
  activate(nodes) |> 
  mutate(group_louvain = as.factor(group_louvain()),
         group_leiden = as.factor(group_leiden()))
rstats_tags_network_communities |> 
  as_tibble()
```

Here is the network grouped by Louvain:

```{r}
rstats_tags_network_communities |> 
  slice_max(order_by = n, n = 100) |> 
  ggraph(layout = "mds") +
  geom_edge_link() +
  geom_node_label(aes(label = name, fill = group_louvain), show.legend = FALSE) +
  theme_graph() +
  coord_equal(clip = "off")
```

And here it is grouped by the Leiden algorithm:

```{r}
rstats_tags_network_communities |> 
  slice_max(order_by = n, n = 100) |> 
  ggraph(layout = "mds") +
  geom_edge_link() +
  geom_node_label(aes(label = name, fill = group_leiden), show.legend = FALSE) +
  theme_graph() +
  coord_equal(clip = "off")
```


## Follower Network

Within the #rstats content, we can also check the community that posts this content.
We first get the user handles from the data:

```{r}
rstats_users <- rstats_content |> 
  distinct(author_handle)
```

Now we query who follows these users who post about #rstats:

```{r follower_data}
#| eval: !expr '!file.exists("data/follower_data.rds")'
follower_data <- rstats_users |> 
  mutate(followed_by = map(author_handle, function(a) {
    # not sure why, but some handles errored when I ran this
    try(get_followers(a, verbose = FALSE)$actor_handle, silent = TRUE)
  }, .progress = TRUE))
saveRDS(follower_data, "data/follower_data.rds")
```

```{r load_follower_data}
follower_data <- readRDS("data/follower_data.rds")
```

Who follows the most accounts contributing to #rstats?

```{r}
follower_data |> 
  unnest_longer(followed_by) |> 
  count(followed_by) |> 
  slice_max(order_by = n, n = 10) |> 
  mutate(followed_by = fct_reorder(followed_by, n)) |> 
  ggplot(aes(x = n, y = followed_by)) +
  geom_col()
```

We can turn this data into a directed network where the nodes are users and the edges are whether a user follows another:

```{r}
follower_network <- follower_data |> 
  filter(map_lgl(followed_by, function(res) !is(res, "try-error"))) |> 
  unnest_longer(followed_by, values_to = "to") |> 
  rename(from = author_handle) |> 
  tbl_graph(edges = _, directed = TRUE)
```

```{r}
follower_network |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  geom_node_point() +
  theme_graph()
```

This is a mess, since there are so many nodes in the network now.
We can again look for the most central nodes (aka users) in this network:

```{r}
follower_network_central <- follower_network |> 
  mutate(
    degree = centrality_degree(),
    closeness = centrality_closeness(),
    betweeness = centrality_betweenness(),
    eigenvector = centrality_eigen()
  )

follower_network_central |>
  arrange(-degree) |> 
  as_tibble()
```

Now let's try only the most central accounts:

```{r}
most_central <- follower_network_central |> 
  slice_max(eigenvector, n = 5) |> 
  activate(nodes) |> 
  as_tibble()

follower_network_central |> 
  slice_max(eigenvector, n = 100) |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  geom_node_point(color = "firebrick") +
  geom_node_label(aes(label = ifelse(name %in% most_central$name,
                                     name,
                                     NA))) +
  theme_graph()
```




## Mention Network

We can also contruct a network with users as nodes and mentions as the edges.
For this, let's extract mentions first:

```{r}
rstats_mentions <- rstats_content |> 
  filter(str_detect(text, "@")) |> 
  mutate(mentions = str_extract_all(text, "@\\w+")) |> 
  select(from = author_handle, to = mentions) |> 
  unnest_longer(to)
```

Let's see who is mentioned most:

```{r}
rstats_mentions |> 
  count(to) |>
  slice_max(order_by = n, n = 10) |> 
  mutate(to = fct_reorder(to, n)) |> 
  ggplot(aes(x = n, y = to)) +
  geom_col()
```

The network comes natural at this point:

```{r}
rstats_mentions_netowrk <- tbl_graph(edges = rstats_mentions, directed = TRUE)
rstats_mentions_netowrk |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  geom_node_point() +
  theme_graph() +
  labs(title = "mention network in #rstats posts")
```


## Share/repost Network

Another way to contruct a network from this data is by looking for reposts.
Here the nodes would be posts

```{r}
#| eval: !expr '!file.exists("data/repost_data.rds")'
repost_data <- rstats_content |> 
  filter(reply_count > 0) |> 
  mutate(
    reposts = map(uri, function(u) {
      get_reposts(u, limit = Inf, verbose = FALSE)
    }, .progress = TRUE),
    reposted_by = map(reposts, "actor_handle")
  )
saveRDS(repost_data, "data/repost_data.rds")
```

```{r load_repost}
repost_data <- readRDS("data/repost_data.rds")
```


```{r repost_network}
repost_network <- repost_data |> 
  unnest_longer(reposted_by, values_to = "to") |> 
  rename(from = author_handle) |> 
  tbl_graph(edges = _, directed = TRUE)
```

```{r}
repost_network |> 
  ggraph(layout = "mds") +
  geom_edge_link() +
  geom_node_point(color = "firebrick") +
  theme_graph()
```

Let's look at cetrality again to see

```{r}
repost_network_central <- repost_network |> 
  mutate(
    degree = centrality_degree(),
    closeness = centrality_closeness(),
    betweeness = centrality_betweenness(),
    eigenvector = centrality_eigen()
  )

repost_network_central |>
  arrange(-degree) |> 
  as_tibble()
```

Let's once again see which are the most central accounts:

```{r}
most_central <- repost_network_central |> 
  slice_max(degree, n = 5) |> 
  activate(nodes) |> 
  as_tibble()

repost_network_central |> 
  slice_max(degree, n = 100) |> 
  ggraph(layout = "stress") +
  geom_edge_link() +
  geom_node_point(color = "firebrick") +
  geom_node_label(aes(label = ifelse(name %in% most_central$name,
                                     name,
                                     NA))) +
  theme_graph()
```

Let's see if we can find any sensible communities in this network:

```{r}
repost_network_central |> 
  activate(nodes) |>
  to_undirected() |> 
  mutate(group_louvain = group_louvain(),
         group_leiden = group_leiden()) |> 
  ggraph(layout = "mds") +
  geom_edge_link() +
  geom_node_point(aes(color = group_louvain)) +
  geom_node_label(aes(label = ifelse(name %in% most_central$name,
                                     name,
                                     NA))) +
  theme_graph()
```

